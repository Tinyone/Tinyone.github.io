<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer | 闲庭品趣</title><meta name="author" content="Dingchao Ren"><meta name="copyright" content="Dingchao Ren"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer自从 2017 年 Google 发布一篇名为《Attention is all you need》的论文，基于transformer的网络模型便层出不穷，特别是以 Bert 和 GPT 为代表的一些语言模型实现了在多种NLP领域任务上超前的性能 ，更是将transformer的热度带向的顶峰。 2023年，openAI推出的基于GPT4的ChatGPT，其在多个领域都展现出">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="https://lovefamily-ren-wang.com/2024/03/31/Transformer/index.html">
<meta property="og:site_name" content="闲庭品趣">
<meta property="og:description" content="Transformer自从 2017 年 Google 发布一篇名为《Attention is all you need》的论文，基于transformer的网络模型便层出不穷，特别是以 Bert 和 GPT 为代表的一些语言模型实现了在多种NLP领域任务上超前的性能 ，更是将transformer的热度带向的顶峰。 2023年，openAI推出的基于GPT4的ChatGPT，其在多个领域都展现出">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lovefamily-ren-wang.com/2024/03/31/Transformer/cover.jpg">
<meta property="article:published_time" content="2024-03-31T12:13:24.000Z">
<meta property="article:modified_time" content="2024-04-14T15:20:49.363Z">
<meta property="article:author" content="Dingchao Ren">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lovefamily-ren-wang.com/2024/03/31/Transformer/cover.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lovefamily-ren-wang.com/2024/03/31/Transformer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-14 23:20:49'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="闲庭品趣" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/head_img.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2024/03/31/Transformer/cover.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="闲庭品趣"><span class="site-name">闲庭品趣</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-31T12:13:24.000Z" title="发表于 2024-03-31 20:13:24">2024-03-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-14T15:20:49.363Z" title="更新于 2024-04-14 23:20:49">2024-04-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>自从 2017 年 Google 发布一篇名为《Attention is all you need》的论文，基于transformer的网络模型便层出不穷，特别是以 Bert 和 GPT 为代表的一些语言模型实现了在多种NLP领域任务上超前的性能 ，更是将transformer的热度带向的顶峰。</p>
<p>2023年，openAI推出的基于GPT4的ChatGPT，其在多个领域都展现出高水平的智能，更是让人们相信基于Transformer这条路可以实现强人工智能。</p>
<h1 id="Transformer-结构"><a href="#Transformer-结构" class="headerlink" title="Transformer 结构"></a>Transformer 结构</h1><p><img src="/2024/03/31/Transformer/Transformer-arch.png"></p>
<p>Transformer主要是由Encoder和Decoder组成，这两部分的结构很相似，主要构成成分是注意力层（Attention Layer）。本文主要是对Transformer这一结构做出一定介绍，网上其实已经有很多介绍了，但很多时候并不是很全面和清晰，笔者在学习的时候发现想找一个全面介绍并不是很容易。</p>
<h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p>对于LLM（large language model）其本质的输入是一段文本，然后输出也是一段文本，我们的目的就是学习一个文本到文本的映射$\textbf{Y} &#x3D; f(\textbf{X})$。当然对于计算机来说，它并不能直接处理文字信息，于是我们需要对输入文本进行转换，即把输入文本变成数字（其实是变成一系列词向量），这一过程分为两个步骤：1. 分词，2. 词嵌入。</p>
<h3 id="分词（Tokenize）"><a href="#分词（Tokenize）" class="headerlink" title="分词（Tokenize）"></a>分词（Tokenize）</h3><p>分词顾名思义就是将一段文本切分成一个一个“词语”，这里一个一个“词语”就叫做Token（也可以翻译为词元），每一个token用一个整数来表示，这种形成token和整数一一对应的形式就叫做词表。比如下面就是一个简单的词表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tokens</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&#x27;The&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&#x27;quick&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;brown&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&#x27;fox&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&#x27;jumps&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">&#x27;over&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&#x27;the&#x27;</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">&#x27;lazy&#x27;</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">&#x27;dog.&#x27;</span>: <span class="number">8</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对人类来说分词似乎是一个自然的过程，比如：“I like eating ice cream.”，一个自然的分词结果就是【’I’, ‘ ‘, ‘like’, ‘ ‘, ‘eating’, ‘ ‘, ‘ice’, ‘ ‘, ‘cream’, ‘.’】，这种分词方法被称为<strong>word base分词</strong>。</p>
<p>但这种分词方式有一个明显的弊端，以英语为例，英语单词往往存在单复数，各种时态，主被动等等不同的形式，这样就需要对所有这些单词都进行表示，那么就会导致最终的词表特别大，同时也会让两个本身意义很接近的词分成两个不同的数，例如：cat和cats。</p>
<p>第二个很容易想到的分词方法是<strong>character base分词</strong>。顾名思义，将每一个字母和标点都作为一个token。首先这种分词方法，词表大小肯定不会很大。以英文为例，字母一共就26个，加上大小写，数字，标点，特殊符号等等，总共也不会很多。但这种分词同样也是有弊端的，这种分词，每一个token缺乏单词的语义信息，并且分词的结果较长，增加了文本表征的成本。</p>
<p>所以现在广泛使用的是上述两种分词方法的折中，即<strong>subword base分词器</strong>。举例来说，“I like eating ice cream.”可能被分词为【’I’, ‘ ‘, ‘like’, ‘ ‘, ‘eat’, ‘ing’, ‘ ‘, ‘ice’, ‘ ‘, ‘cream.’】，最明显的区别就是eating被拆分成两个subword：eat和ing，这样做的好处是两个token都有一定意义，同时也可以缩减词表大小，例如：eat，eating，drink，drinking，按照word base进行分词，需要4个不同数来表示；如果是subword base分词，则只需要3个数来表示，eat，drink，ing。</p>
<p>subword base分词器可以说是现在大模型分词的主流分词方法，不过值得一提的是，如果单纯对中文进行分词还是更多地使用character base分词，这是因为中文每一个字是有明确意义的，且不存在多种表现形式（单复数等等这种），这样词表大小也不会很大。</p>
<p>常见的subword base分词算法有Byte Pair Encoding (BPE)，openAI从GPT2开始就一直使用这种分词方法。bert使用的是wordpiece算法，可以看做是BPE算法的一种变种，另外还有Unigram LM算法。这些算法这里后续有机会再介绍。</p>
<h3 id="词嵌入（word-embedding）"><a href="#词嵌入（word-embedding）" class="headerlink" title="词嵌入（word embedding）"></a>词嵌入（word embedding）</h3><p>结合上述分词的介绍，我们知道输入文本，根据词表，转化为一串数字（这个往往是分词器的功能），那么词嵌入就是将每一个数字映射到一个高维空间，每一个数字用一个向量来表示。一个朴素的想法就是用one-hot进行编码。举例来说，一个token的id&#x3D;2，那么就用$e_2 &#x3D; [0, 1, 0,\ldots, 0]$这个单位向量进行表示，$e_i \in \mathbf{R}^n$，$n$是词表大小。但这种编码有很大的问题，就是这些高维向量丢失了token本身的含义，比如对于很多意思相近的token，我们也期望它们在高维空间里的向量表示比较接近，同时这种one-hot编码有维度灾难的问题。除了这种强稀疏性的编码，我们可以选择稠密的编码方式，这种编码如何实现呢？这里暂时不进行详细介绍，它同样很重要，但不介绍不会影响后面的理解。</p>
<p>简单来说，我们会得到一个embedding矩阵$M\in \mathbf{R}^{n\times d}$，$n$是词表大小，$d$是高维向量的维度。根据词表中的token id能查到对应的词向量。</p>
<p><strong>Remark：</strong>这里有个认知需要澄清一下，虽然上面一直说用一个高维向量来表示这个token id，其实这种说法不是很合适。token id本身没有意义，它对应的那个token才是有意义的，而现实中所有的token一起构成的是一个真正的高维空间。相比token本身这个空间，最后那个embedding向量其实是一个低维的，所以这里使用embedding这个词。embedding本身的意思是嵌入，就是将一个高维的token嵌入到一个低维空间，用一个$d$维向量来表示。</p>
<h2 id="位置编码（position-encoding）"><a href="#位置编码（position-encoding）" class="headerlink" title="位置编码（position encoding）"></a>位置编码（position encoding）</h2><p>经过上面介绍，我们知道输入一段文本，经过分词和embedding会得到一个$s \times d$为的矩阵，其中$s$是分词后的长度，$d$是embedding维度。</p>
<p>这里有一个小问题，不同的文本得到的矩阵其行维度是不一样的，这该如何解决呢？其实也很简单，假设Transformer接受的输入长度一个定值“seq_length&#x3D;2048”，那么对于行数不足2048的矩阵进行补齐操作就行（可以补0，也可以是补其他的），超过的进行截断。</p>
<p>那么什么是位置编码？为什么需要位置编码？</p>
<p>首先回答第一个问题。位置编码，顾名思义：对序列中的词的位置进行编码。有些地方也称为position embedding。</p>
<p>现在回答第二个问题。对于任何一门语言，单词在句子中的位置以及排列顺序是非常重要的，它们不仅是一个句子的语法结构的组成部分，更是表达语义的重要概念。一个单词在句子的位置或排列顺序不同，可能整个句子的意思就发生了偏差。</p>
<ul>
<li>I <strong>do not</strong> like the story of the movie, but I <strong>do</strong> like the cast.</li>
<li>I <strong>do</strong> like the story of the movie, but I <strong>do not</strong> like the cast.</li>
</ul>
<p>上面两句话所使用的的单词完全一样，但是所表达的句意却截然相反。那么，引入词序信息有助于区别这两句话的意思。</p>
<p>Transformer模型抛弃了RNN、CNN作为序列学习的基本模型。我们知道，循环神经网络本身就是一种顺序结构，天生就包含了词在序列中的位置信息。当抛弃循环神经网络结构，完全采用Attention取而代之，这些词序信息就会丢失，模型就没有办法知道每个词在句子中的相对和绝对的位置信息。因此，有必要把词序信号加到词向量上帮助模型学习这些信息，位置编码就是用来解决这种问题的方法。</p>
<p>现在最常用的位置编码是RoPE，也称旋转位置编码，RoPE 具有更好的外推性，这促进了对长上下文窗口的支持，这个课题日后有机会再聊。这里给出几篇相关的介绍：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/tardis/zm/art/647109286?source_id=1005">十分钟读懂旋转编码（RoPE）</a></li>
<li><a target="_blank" rel="noopener" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">Transformer Architecture: The Positional Encoding</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/106644634">一文读懂Transformer模型的位置编码</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/134085503">一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long</a></li>
</ul>
<p>将位置编码和输入文本的词向量相加，便可以得到Encoder层的输入了。</p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Transformer的一个Encoder-layer主要由两部分组成：Attention和FFN（Feedforward networks），有时候也称为MLP（Multilayer Perceptron，多层感知机）。</p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>Attention的实现方式有很多，最常见的还是scaled dot product attention</p>
<p><img src="/2024/03/31/Transformer/attention.png"></p>
<p>这里先主要对attention的数学计算进行介绍。</p>
<p>首先attention的输入是$X\in \mathbf{R}^{s\times d}$，其中$s$是输入的token长度，$d$是embedding维度。通过三个矩阵$W_Q$，$W_K$，$W_V$，将输入$X$映射成query：$Q$，key：$K$和value：$V$矩阵。<br>$$<br>Q &#x3D; XW_Q, \quad<br>K &#x3D; XW_K, \quad<br>V &#x3D; XW_V \tag{1}<br>$$</p>
<p>$W_Q$，$W_K$，$W_V$的维度分别是：$d \times d_q$，$d \times d_k$，$d \times d_v$。一般情况下$d_q &#x3D; d_k &#x3D; d_v &#x3D; d$（维度不一样也可以，只要符合矩阵计算的维度即可）。<br>这样得到的$Q$, $K$, $V$的维度分别是$s\times d$，Scaled Dot-product Attention 计算公式如下：<br>$$<br>Attention(Q,K,V) &#x3D; softmax\left(\frac{Q K^{\top}}{\sqrt{d}}\right)V \tag{2}<br>$$<br>这里的softmax是按最后一维进行归一化（即行和为1）。Attention计算的结果同样是一个矩阵，维度为$s\times d$。</p>
<h3 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h3><p>对与现在的大模型，大多使用多头注意力机制，即，Multi-head Attention（MHA）。MHA首先通过线性映射将$Q,K,V$序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention：<br><img src="/2024/03/31/Transformer/multi_head_attention.png"></p>
<p>每个注意力头负责关注某一方面的语义相似性，多个头就可以让模型同时关注多个方面。因此与简单的 Scaled Dot-product Attention 相比，Multi-head Attention 可以捕获到更加复杂的特征信息。<br>$$<br>head_i &#x3D; \text{Attention}(Q_i, K_i, V_i)<br>$$<br>其中$Q_i, K_i, V_i$的维度均是$s \times d_i$，因为$Q_i, K_i, V_i$是由$X$乘以对应的矩阵$W_{Q_i}, W_{K_i}, W_{V_i}$得到的，所以这里假设了三个矩阵维度都是$d \times d_i$。那么MultiHead就将每个$head_i$拼起来，即：<br>$$<br>\text{MultiHead}(Q,K,V) &#x3D; \text{Concat}(head_1, \ldots, head_h)<br>$$<br>一般情况下，$d_i$会设定为一个常数，即$d_i &#x3D; \bar{d}_k$，MultiHead的计算结果是一个$s \times h\bar{d}_k$的矩阵（Concat是按行拼接）。往往也会令$h\bar{d}_k &#x3D; d$，这样MultiHead的结算结果和单个head计算的结果维度就完全一样了。例如在标准的BERT里面，embedding维度$d &#x3D; 768$，head个数$h&#x3D;12$，这样每个head attention 维度$\bar{d}_k &#x3D; 768 &#x2F; 12 &#x3D; 64$。事实上，即使MultiHead的输出维度不等于$s\times d$也没问题，后面在接一个线性变换即可。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://lovefamily-ren-wang.com">Dingchao Ren</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lovefamily-ren-wang.com/2024/03/31/Transformer/">https://lovefamily-ren-wang.com/2024/03/31/Transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://lovefamily-ren-wang.com" target="_blank">闲庭品趣</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="/2024/03/31/Transformer/cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/08/07/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/" title="数值优化"><img class="cover" src="/2023/08/07/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数值优化</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/head_img.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dingchao Ren</div><div class="author-info__description">请走慢一点，给自己一点喘息</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Tinyone"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">rendingchao@icloud.com</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer"><span class="toc-number">1.</span> <span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer-%E7%BB%93%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">Transformer 结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5"><span class="toc-number">2.1.</span> <span class="toc-text">输入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E8%AF%8D%EF%BC%88Tokenize%EF%BC%89"><span class="toc-number">2.1.1.</span> <span class="toc-text">分词（Tokenize）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%EF%BC%88word-embedding%EF%BC%89"><span class="toc-number">2.1.2.</span> <span class="toc-text">词嵌入（word embedding）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88position-encoding%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">位置编码（position encoding）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder"><span class="toc-number">2.3.</span> <span class="toc-text">Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention"><span class="toc-number">2.3.1.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-head-attention"><span class="toc-number">2.3.2.</span> <span class="toc-text">Multi-head attention</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/03/31/Transformer/" title="Transformer"><img src="/2024/03/31/Transformer/cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer"/></a><div class="content"><a class="title" href="/2024/03/31/Transformer/" title="Transformer">Transformer</a><time datetime="2024-03-31T12:13:24.000Z" title="发表于 2024-03-31 20:13:24">2024-03-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/07/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/" title="数值优化"><img src="/2023/08/07/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数值优化"/></a><div class="content"><a class="title" href="/2023/08/07/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/" title="数值优化">数值优化</a><time datetime="2023-08-07T14:15:45.000Z" title="发表于 2023-08-07 22:15:45">2023-08-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="深度学习环境搭建"><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深度学习环境搭建"/></a><div class="content"><a class="title" href="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="深度学习环境搭建">深度学习环境搭建</a><time datetime="2023-07-02T05:27:14.000Z" title="发表于 2023-07-02 13:27:14">2023-07-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/07/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/" title="深度学习-深度前馈网络"><img src="/2023/07/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/wallhaven-gp2jqd.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深度学习-深度前馈网络"/></a><div class="content"><a class="title" href="/2023/07/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/" title="深度学习-深度前馈网络">深度学习-深度前馈网络</a><time datetime="2023-07-01T06:30:25.000Z" title="发表于 2023-07-01 14:30:25">2023-07-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Dingchao Ren</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>