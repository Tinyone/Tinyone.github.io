<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>闲庭品趣</title>
  
  
  <link href="https://lovefamily-ren-wang.com/atom.xml" rel="self"/>
  
  <link href="https://lovefamily-ren-wang.com/"/>
  <updated>2024-04-01T14:56:19.639Z</updated>
  <id>https://lovefamily-ren-wang.com/</id>
  
  <author>
    <name>Dingchao Ren</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer</title>
    <link href="https://lovefamily-ren-wang.com/2024/03/31/Transformer/"/>
    <id>https://lovefamily-ren-wang.com/2024/03/31/Transformer/</id>
    <published>2024-03-31T12:13:24.000Z</published>
    <updated>2024-04-01T14:56:19.639Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>自从 2017 年 Google 发布一篇名为《Attention is all you need》的论文，基于transformer的网络模型便层出不穷，特别是以 Bert 和 GPT 为代表的一些语言模型实现了在多种NLP领域任务上超前的性能 ，更是将transformer的热度带向的顶峰。</p><p>2023年，openAI推出的基于GPT4的ChatGPT，其在多个领域都展现出高水平的智能，更是让人们相信基于Transformer这条路可以实现强人工智能。</p><h1 id="Transformer-结构"><a href="#Transformer-结构" class="headerlink" title="Transformer 结构"></a>Transformer 结构</h1><p><img src="/2024/03/31/Transformer/Transformer-arch.png"></p><p>Transformer主要是由Encoder和Decoder组成，这两部分的结构很相似，主要构成成分是注意力层（Attention Layer）。本文主要是对Transformer这一结构做出一定介绍，网上其实已经有很多介绍了，但很多时候并不是很全面和清晰，笔者在学习的时候发现想找一个全面介绍并不是很容易。</p><h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p>对于LLM（large language model）其本质的输入是一段文本，然后输出也是一段文本，我们的目的就是学习一个文本到文本的映射$\textbf{Y} &#x3D; f(\textbf{X})$。当然对于计算机来说，它并不能直接处理文字信息，于是我们需要对输入文本进行转换，即把输入文本变成数字（其实是变成一系列词向量），这一过程分为两个步骤：1. 分词，2. 词嵌入。</p><h3 id="分词（Tokenize）"><a href="#分词（Tokenize）" class="headerlink" title="分词（Tokenize）"></a>分词（Tokenize）</h3><p>分词顾名思义就是将一段文本切分成一个一个“词语”，这里一个一个“词语”就叫做Token（也可以翻译为词元），每一个token用一个整数来表示，这种形成token和整数一一对应的形式就叫做词表。比如下面就是一个简单的词表：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tokens</span><br><span class="line">&#123;</span><br><span class="line">    &#x27;The&#x27;: 0,</span><br><span class="line">    &#x27;quick&#x27;: 1,</span><br><span class="line">    &#x27;brown&#x27;: 2,</span><br><span class="line">    &#x27;fox&#x27;: 3,</span><br><span class="line">    &#x27;jumps&#x27;: 4,</span><br><span class="line">    &#x27;over&#x27;: 5,</span><br><span class="line">    &#x27;the&#x27;: 6,</span><br><span class="line">    &#x27;lazy&#x27;: 7,</span><br><span class="line">    &#x27;dog.&#x27;: 8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对人类来说分词似乎是一个自然的过程，比如：“I like eating ice cream.”，一个自然的分词结果就是【’I’, ‘ ‘, ‘like’, ‘ ‘, ‘eating’, ‘ ‘, ‘ice’, ‘ ‘, ‘cream’, ‘.’】，这种分词方法被称为<strong>word base分词</strong>。</p><p>但这种分词方式有一个明显的弊端，以英语为例，英语单词往往存在单复数，各种时态，主被动等等不同的形式，这样就需要对所有这些单词都进行表示，那么就会导致最终的词表特别大，同时也会让两个本身意义很接近的词分成两个不同的数，例如：cat和cats。</p><p>第二个很容易想到的分词方法是<strong>character base分词</strong>。顾名思义，将每一个字母和标点都作为一个token。首先这种分词方法，词表大小肯定不会很大。以英文为例，字母一共就26个，加上大小写，数字，标点，特殊符号等等，总共也不会很多。但这种分词同样也是有弊端的，这种分词，每一个token缺乏单词的语义信息，并且分词的结果较长，增加了文本表征的成本。</p><p>所以现在广泛使用的是上述两种分词方法的折中，即<strong>subword base分词器</strong>。举例来说，“I like eating ice cream.”可能被分词为【’I’, ‘ ‘, ‘like’, ‘ ‘, ‘eat’, ‘ing’, ‘ ‘, ‘ice’, ‘ ‘, ‘cream.’】，最明显的区别就是eating被拆分成两个subword：eat和ing，这样做的好处是两个token都有一定意义，同时也可以缩减词表大小，例如：eat，eating，drink，drinking，按照word base进行分词，需要4个不同数来表示；如果是subword base分词，则只需要3个数来表示，eat，drink，ing。</p><p>subword base分词器可以说是现在大模型分词的主流分词方法，不过值得一提的是，如果单纯对中文进行分词还是更多地使用character base分词，这是因为中文每一个字是有明确意义的，且不存在多种表现形式（单复数等等这种），这样词表大小也不会很大。</p><p>常见的subword base分词算法有Byte Pair Encoding (BPE)，openAI从GPT2开始就一直使用这种分词方法。bert使用的是wordpiece算法，可以看做是BPE算法的一种变种，另外还有Unigram LM算法。这些算法这里后续有机会再介绍。</p><h3 id="词嵌入（word-embedding）"><a href="#词嵌入（word-embedding）" class="headerlink" title="词嵌入（word embedding）"></a>词嵌入（word embedding）</h3><p>结合上述分词的介绍，我们知道输入文本，根据词表，转化为一串数字（这个往往是分词器的功能），那么词嵌入就是将每一个数字映射到一个高维空间，每一个数字用一个向量来表示。一个朴素的想法就是用one-hot进行编码。举例来说，一个token的id&#x3D;2，那么就用$e_2 &#x3D; [0, 1, 0,\ldots, 0]$这个单位向量进行表示，$e_i \in \mathbf{R}^n$，$n$是词表大小。但这种编码有很大的问题，就是这些高维向量丢失了token至今本身的含义，比如对于很多意思相近的token，我们也期望它们在高维空间里的向量表示比较接近，同时这种one-hot编码有维度灾难的问题。除了这种强稀疏性的编码，我们可以选择稠密的编码方式，这种编码如何实现呢？这里暂时不进行详细介绍，它同样很重要，但不介绍不会影响后面的理解。</p><p>简单来说，我们会得到一个embedding矩阵$M\in \mathbf{R}^{n\times d}$，$n$是词表大小，$d$是高维向量的维度。根据词表中的token id能查到对应的词向量。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Transformer&quot;&gt;&lt;a href=&quot;#Transformer&quot; class=&quot;headerlink&quot; title=&quot;Transformer&quot;&gt;&lt;/a&gt;Transformer&lt;/h1&gt;&lt;p&gt;自从 2017 年 Google 发布一篇名为《Attention </summary>
      
    
    
    
    
    <category term="LLM" scheme="https://lovefamily-ren-wang.com/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>数值优化</title>
    <link href="https://lovefamily-ren-wang.com/2023/08/07/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/"/>
    <id>https://lovefamily-ren-wang.com/2023/08/07/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/</id>
    <published>2023-08-07T14:15:45.000Z</published>
    <updated>2023-08-13T18:07:48.500Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数值优化"><a href="#数值优化" class="headerlink" title="数值优化"></a>数值优化</h1><p>无论是机器学习还是深度学习，本质上都是求解一个输入到输出的最好的映射，那么这里的“最好”的定义根据不同场景，任务目标也会有所不同。给出“最好”的定义之后，那么就需要去求解最优的映射，这个求解的过程，就是数值优化。本篇博客主要是介绍数值优化的一些基础概率和基本算法，更为高深的数值算法，这里不在赘述。</p><h2 id="上溢和下溢"><a href="#上溢和下溢" class="headerlink" title="上溢和下溢"></a>上溢和下溢</h2><p>严格来说，在计算机里所有的数都是离散的，并且是有限的，其离散化的精度和所表示数的范围，都是有限的。举个例子来说，用64个比特位来表示整数，其能表示的整数范围为[-9223372036854775808, 9223372036854775807]，虽然这个范围很大，但并不是无穷的。同理如果用64个个比特位来表示浮点数，那么所能表示的浮点数个数也是有限的，所以这里就存在舍入误差。</p><p>为了更直观的看到舍入误差，这里举一个例子，以4个比特位为例子，前两个表示浮点数的整数部分，后两个比特位来表示浮点数的小数部分。比如1.2就可以用0110来表示，其中01是整数部分1的二进制表示，而后面的10则是小数部分2的二进制表示。但是这样的4个比特位，其表示的浮点数的能力非常有限。因为我们很容易就可以看到，它只能表示小数部分为0,1,2,3的小数，其余的表示不了，比如1.5，1.21。这里表示值和真实的浮点数之间的误差，可以称之为舍入误差。所以无论是用多少个比特位来表示浮点数，只要比特位个数有限，就总会存在误差。</p><p>当然这里的舍入误差一般不会很大，毕竟64个比特位已经能满足日常计算的需求。不过遗憾的是，舍入误差会导致一些问题，特别是当许多操作复合时，即使是理论上可行的算法，如果在设计时没有考虑最小化舍入误差的累积，在实践时也可能会导致算法失效。</p><p>一种比较麻烦的舍入误差是下溢（underflow），当接近零的数被四舍五入为<br>零时发生下溢。许多函数在其变量为0和很小的正数是所表现出的性质完全不同，比如在做除法时，分母不能0。除零之后会是一个“无穷大的数”，这时一些软件会抛出异常，或者返回一个非数字 (not-a-number, NaN) 的占位符。这样进一步的计算就无法进行了。</p><p>另一个极具破坏力的数值错误形式是上溢（overflow）。当大量级的数被近似为∞ 或 −∞ 时发生上溢。进一步的运算通常会导致这些无限值变为非数字。</p><p>上溢和下溢都会造成数值问题，这里举一个例子来更好的理解。在神经网络中，尤其是分类问题，softmax函数经常使用。softmax函数的定义为</p><p>$$<br>softmax(x)<em>i &#x3D; \frac{\exp(x_i)}{\sum</em>{j&#x3D;1}^n\exp(x_j)} \tag{1}<br>$$</p><p>如果$x$的所有分量都是一个常数$c$，那么softmax函数的值应该是一个$1&#x2F;n$<br>然而，实际上当常数$c$比较大时，softmax函数的分子分母都是一个很大数（指数爆炸），这样的数早就超出了计算机的表示范围，于是出现了上溢，那么计算机便无法计算出此时softmax的函数值。同理如果此时c是一个比较小的负数，那么此时分子分母又会很接近0，此时又容易出现下溢，那么这个结果也是未定义的。这两个困难可以通过一个简单的变换来解决，我们放弃计算$softmax(x)$转为计算$softmax(z)$，其中$z &#x3D; x - max_i x_i$。这样的变换可以保证，即使$x_i$ 很大，$z$最大值也只是0，而且分母总是保证有一个分量为1。</p><p>在大多数情况下，我们没有明确地对本书描述的各种算法所涉及的数值考虑进行详细说明。底层库的开发者在实现深度学习算法时应该牢记数值问题。本书的大多数读者可以简单地依赖保证数值稳定的底层库。在某些情况下，我们有可能在实现一个新的算法时自动保持数值稳定。<a href="https://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf">Theano (Bergstra et al., 2010a; Bastien et al.,2012a)</a> 就是这样软件包的一个例子，它能自动检测并稳定深度学习中许多常见的数值不稳定的表达式。</p><h2 id="病态条件"><a href="#病态条件" class="headerlink" title="病态条件"></a>病态条件</h2><p>无论是深度学习还是机器学习，我们在求解最优解的时候，常常会涉及到矩阵运算，而且这其中就包括矩阵求逆运算。一个$n\times n$的矩阵求逆运算的时间复杂度是$\mathcal{O}(n^3)$。然后矩阵求逆不仅仅是复杂度高，还会受矩阵病态的影响，导致求逆运算对很小的误差或者噪声非常敏感。举个例子来说。</p><p>现在有一个线性方程$Ax &#x3D; b$,<br>$$<br> \begin{bmatrix}<br>   400 &amp; -201 \<br>   -800 &amp; 401<br>  \end{bmatrix} \begin{bmatrix}<br>   x_1  \<br>    x_2<br>  \end{bmatrix} &#x3D; \begin{bmatrix}<br>   200  \<br>   -200<br>  \end{bmatrix} \tag{2}<br>$$<br>上述方程的解为$x_1 &#x3D; -100$, $x_2 &#x3D; -200$，但是如果现在出现了一点扰动，导致矩阵$A$的第一个元素从400变为401，即,<br>$$<br> \begin{bmatrix}<br>   401 &amp; -201 \<br>   -800 &amp; 401<br>  \end{bmatrix} \begin{bmatrix}<br>   x_1  \<br>    x_2<br>  \end{bmatrix} &#x3D; \begin{bmatrix}<br>   200  \<br>   -200<br>  \end{bmatrix} \tag{3}<br>$$<br>此时方程的解会变为$x_1 &#x3D; 40000$, $x_2 &#x3D; 79800$。矩阵$A$一个小小的变化，会导致方程的解发生巨大变化。说明了此时方程的解对参数非常敏感，而这个矩阵也称为病态的（ill conditioned）。</p><p>矩阵病态是一个很难避免的问题，因为在实际工程应用中，求解线性方程组$Ax&#x3D;b$问题时，其系数矩阵或初值矩阵 ​ 一般由实验数据构成，如果矩阵$A$或$b$中的元素存在观测误差、仪器的测量误差或计算机本身的误差等，则会导致求得解偏离真实值很多。</p><p>那么如何去衡量一个矩阵是否是病态的呢，数学根据条件数，给出了如下结论：考虑函数$f(x) &#x3D; A^{-1}x$，其中$A\in \mathbb{R}^{n\times n}$的特征值为$\lambda_i, i&#x3D;1, \dots, n$，其条件数为<br>$$<br>\kappa(A) &#x3D; \max_{i,j} \mid\frac{\lambda_i}{\lambda_j}\mid \tag{4}<br>$$<br>这是最大和最小特征值的模之比，当该数很大时，矩阵求逆对输入的误差特别敏感。当该数很大时，矩阵求逆对输入的误差特别敏感。<br>这种敏感性是矩阵本身的固有特性，而不是矩阵求逆期间舍入误差的结果。也就是说即使我们乘以完全正确的矩阵逆，病态条件的矩阵也会放大预先存在的误差。在实践中，该错误将与求逆过程本身的数值误差进一步复合。</p><h2 id="基于梯度的优化方法"><a href="#基于梯度的优化方法" class="headerlink" title="基于梯度的优化方法"></a>基于梯度的优化方法</h2><p>之前的章节已经提到了，绝大多数深度学习或者机器学习目标都是求解一个最优的映射，这其中都涉及到优化某一个目标函数，即，$\min_{x} f(x)$，其中$x$是自变量，$f(x)$是目标函数（objective function），或者称为准则（criterion），因为这里是采用最小化$f(x)$，所以根据场景也常常称为损失函数（loss function），误差函数（error function），代价函数（cost function）。这里求解出的最优解往往用$x^*$表示，即，$x^* &#x3D; \arg\min_x f(x)$。下面的内容需要一些微积分的基础知识。</p><p>对于一个函数$y &#x3D; f(x)$，$x,y \in \mathbb{R}$都是实数，导数是提供函数变化确实最直接的线索。函数的导数定义为$f^{\prime}(x) &#x3D; \frac{\partial f(x)}{\partial x}$，导数$f^{\prime}(x)$的方向是函数在该点增加最快的方向，$-f^{\prime}(x)$函数减小最快的方向。一个直观的函数优化方法就是，沿着导数的反方向去寻找更优的$x$，这种方法被称为梯度下降法（gradient descent）。这里需要提一句，在$x$是单变量的场景下，梯度和导数没有区别，如果是多变量，比如$x &#x3D; (x_1, x_2)^T$，此时的导数$\frac{\partial{f(x)}}{\partial{x}} &#x3D; \left(\frac{\partial{f(x_1)}}{\partial{x_1}}, \frac{\partial{f(x_2)}}{\partial{x_2}}\right)^T$，这个也称为梯度。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;数值优化&quot;&gt;&lt;a href=&quot;#数值优化&quot; class=&quot;headerlink&quot; title=&quot;数值优化&quot;&gt;&lt;/a&gt;数值优化&lt;/h1&gt;&lt;p&gt;无论是机器学习还是深度学习，本质上都是求解一个输入到输出的最好的映射，那么这里的“最好”的定义根据不同场景，任务目标也会有所</summary>
      
    
    
    
    <category term="数值优化" scheme="https://lovefamily-ren-wang.com/categories/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/"/>
    
    
    <category term="Deep Learning" scheme="https://lovefamily-ren-wang.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习环境搭建</title>
    <link href="https://lovefamily-ren-wang.com/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>https://lovefamily-ren-wang.com/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</id>
    <published>2023-07-02T05:27:14.000Z</published>
    <updated>2023-07-02T10:08:06.101Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本篇博客主要是帮助读者搭建windows下深度学习的环境。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>windows下深度学习的环境搭建主要包括五个部分，分别是显卡驱动，cuda，cudnn，anaconda 和pytorch。</p><h1 id="显卡驱动"><a href="#显卡驱动" class="headerlink" title="显卡驱动"></a>显卡驱动</h1><p>想要搭建深度学习的环境，首先是需要一张显卡（虽然也可用CPU来做训练卡，但是CPU没有为深度学习做优化加速，训练性能远不如GPU）。一般来说Nvida显卡更加普及一些，所以就以Nvida显卡为例。如果电脑里已经安装了显卡驱动(一般都已经装好了)，可以进行第二步cuda安装。</p><p>如何确定是否有显卡驱动呢？非常简单，同事按下WI+X键，再按下M键，或者在电脑左下角搜索框输入设备管理器，打开设备管理器点击显示适配器，如下图所示<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86%E5%99%A8.png" alt="设备管理器"><br>图上显示的NVIDIA GeForce GTX 1660 Ti就是你的显卡，点击右键选择属性，再点击驱动程序一栏，如果可以打开，并显示了版本号，则说明显卡驱动已经安装完成，请移步第二步。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F.png"></p><p>如果打不开或者无法显示则需要安装显卡驱动。可以进入<a href="https://www.nvidia.cn/geforce/drivers/">NVIDA官网下载最新的驱动</a>，在下图中输入你的相关信息，然后开始搜索驱动，选择一个驱动版本进行下载并安装。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/Nvida-driver.png"></p><h1 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h1><p>CUDA(Compute Unified Device Architecture)，是显卡厂商NVIDIA推出的运算平台。 CUDA是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。CUDA给人工智能提供了很好的计算支持。</p><p>在下载cuda之前有一件事必须明确下来，否则后续的安装可能就不那么顺利了。驱动程序版本，CUDA版本，pytorch版本，python版本必须要相互适配。</p><p>首先CUDA要和驱动程序版本匹配，匹配表可以通过官方的CUDA Toolkit版本和显卡驱动对应的表格来查看，网址为：<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id5">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id5</a><br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/docs.nvidia.com_cuda_cuda-toolkit-release-notes_index.png"><br>驱动版本可通过右下角驱动信息来查看，右键点击Nvida图标，选择Nvida控制面板。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E9%A9%B1%E5%8A%A8%E4%BF%A1%E6%81%AF.png"><br>在控制面板中点击系统信息，就可以查看到驱动程序版本(需要记住)。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E6%8E%A7%E5%88%B6%E9%9D%A2%E6%9D%BF.png"></p><p>你的显卡驱动版本号必须大于等于表格中的版本号你才可以安装该版本的CUDA，我这里要安装的是CUDA 11.1 GA，而我的驱动号是457.49&gt;&#x3D;456.38，因此可以安装。</p><p>此时最好去确定11.1 版本所适配的的pytroch和Python版本了，提前确认一下以防对应版本没有（一般来说是有的）。pytorch版本和cuda版本适配可以在<a href="https://pytorch.org/">pytorch官网</a>查看。进入官网可以点击previous versions of pytorch 查看。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/pytorch.png"></p><p>点击previous versions of pytorch，搜索CUDA 11.1 查看对应的所有可支持的版本。需要注意的是，这里有conda和wheel两种安装方式，笔者这里选择的是pytroch1.8.0的conda安装方式，如下图所示。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/torch1.8.0-conda11.1.png"></p><p>最后就是确定一下pytorch1.8.0所需要的python版本，这可以在网上搜一下。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/torch-python.png"><br>可以看出可选的python版本为3.8和3.9，那么至此版本都选定了。</p><table><thead><tr><th align="center"></th><th align="center">版本</th></tr></thead><tbody><tr><td align="center">驱动</td><td align="center">457.49</td></tr><tr><td align="center">Cuda toolkit</td><td align="center">11.1 GA</td></tr><tr><td align="center">pytorch</td><td align="center">1.8.0</td></tr><tr><td align="center">python</td><td align="center">3.9</td></tr></tbody></table><p>下面可以依次下载。进入CUDA下载界面<a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive</a> 选择cuda11.1</p><p><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/cuda11.1.png"><br>然后进入以下界面，选择windows，x86平台，版本10，和local安装，点击右下角下载。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/cudadownload.png"></p><p>下载完成可以进行安装，可精简安装，也可以自定义安装，需要记住安装地址，安装完成后，打开系统环境变量，发现多了两个变量，CUDA_PATH和CUDA_PATH_V11_1。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/cuda-path.png"></p><p>打开cmd，输入nvcc -V，查看安装的cuda版本。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/nvcc-V.png"></p><h1 id="cuDNN"><a href="#cuDNN" class="headerlink" title="cuDNN"></a>cuDNN</h1><p>进入<a href="https://developer.nvidia.com/zh-cn/cudnn"></a>，选择cuda11.1 对应的cuDNN版本进行下载，其实是一个压缩包，解压缩之后将lib，include，lib三个文件夹，复制到cuda文件夹下v11.1文件中<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/cuDNN.png"><br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/v11.1.png"><br>至此cuDNN安装完成</p><h1 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h1><p>Anaconda是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。因为包含了大量的科学包，Anaconda 的下载文件比较大（约 531 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用Miniconda这个较小的发行版（仅包含conda和 Python）。</p><p>进入<a href="https://www.anaconda.com/download">anaconda下载界面</a>。下载完成后，进行安装，建议安装至非c盘，因为后续环境会下载很多安装包，这里要注意。安装过程中，add anaconda3 to my path environment variable 这个勾选的话，就可以直接在cmd用conda命令，但可能会影响其他软件；不勾选的话，可以在开始菜单中打开Anaconda Navigator或Anaconda Prompt，然后在里面用conda等命令。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/anaconda.png"></p><p>然后打开annconda。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/anaconda-prompt.png"><br>这是一个和命令行很像的东西。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/anaconda-cmd.png"><br>Conda 是一个开源的软件包管理系统和环境管理系统，用于安装多个版本的软件包及其依赖关系，并在它们之间轻松切换。</p><p>环境就像是一个工具仓库，当我们做一个项目时会用到很多python的软件包，做另外一个项目时会用到其他的软件包或者是不同版本的软件包，如果把所有软件包都安装在主环境下的话，包与包之间可能会冲突，就好像汽油和火把不能放在同一个仓库里。而这里的base指的是主环境，也就是主仓库，这里我们新建一个虚拟环境，分仓库，用来存放搭建深度学习的所需要的环境。指令为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n torch18_py39 python=3.9</span><br></pre></td></tr></table></figure><p>其中torch18_py39是环境名，python&#x3D;3.9是之前我们选择的python版本。接下来会提供一些基础的软件包，输入y确认安装。然后输入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate torch18_py39</span><br></pre></td></tr></table></figure><p>就进入到了我们新创建的虚拟环境<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/torch18_py39.png"></p><p>可以看到前面base已经变成torch18_py39了，这样我们就从主环境切换到了新的环境了。</p><p>至此，anaconda也完成安装了。</p><h1 id="pytroch"><a href="#pytroch" class="headerlink" title="pytroch"></a>pytroch</h1><p>终于来到最后一步，pytroch安装。在cuda安装的那一节里，我们已经选了pytroch 1.8.0版本。</p><p>首先打开anaconda prompt，进入上一节创建好的torch18_py39虚拟环境。<a href="https://pytorch.org/get-started/previous-versions/">进入pytroch网站</a>，选好版本(pytorch 1.8.0) 复制下面的指令到anaconda的命令行中，回车确认。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/torch1.8.0-conda11.1-download.png"></p><p>有一些小伙伴可能会读取失败或者在安装过程中卡在某个地方不动了，这是因为anaconda是国外源，我们可以将anaconda的channel切换到清华源或者其他的一些国内源。具体的切换方法为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">在anaconda 命令行中分别输入以下命令</span><br><span class="line"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/</span><br><span class="line"> conda config --set show_channel_urls yes </span><br></pre></td></tr></table></figure><p> 然后再下载，下载过程出现的问题可以自行百度。</p><p>pytorch 安装完成后，在anaconda 命令行界面，输入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line">import torch</span><br><span class="line">print(torch.cuda.is_available())</span><br></pre></td></tr></table></figure><p>结果返回True说明安装完成，可以开始炼丹了。<br><img src="/2023/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/torch-is-available.png"></p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p> 这篇安装教程是主要参考这篇文章所写<a href="https://zhuanlan.zhihu.com/p/443147240">Windows环境下pytorch深度学习环境搭建</a>，并增加了一点细节。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本篇博客主要是帮助读者搭建windows下深度学习的环境。&lt;/p&gt;
&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;hea</summary>
      
    
    
    
    <category term="环境搭建" scheme="https://lovefamily-ren-wang.com/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
    <category term="CUDA" scheme="https://lovefamily-ren-wang.com/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-深度前馈网络</title>
    <link href="https://lovefamily-ren-wang.com/2023/07/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/"/>
    <id>https://lovefamily-ren-wang.com/2023/07/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/</id>
    <published>2023-07-01T06:30:25.000Z</published>
    <updated>2023-07-02T05:28:17.667Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>随着alpha-go和Chatgpt的出现，人工智能（Artificial Intelligence，AI）展现出令人惊叹的发展前景，作为一个科技工作者（新时代民工），应该对其要有所了解的。深度学习作为人工智能领域一个非常重要的课题，自然是不能错过的。</p><p>笔者之前也对深度学习了解较少，但工作上遇到了一些问题，发现用深度学习的一些方法可以很好的解决，所以想工作之余系统性的学习一下。<a href="https://www.deeplearningbook.org/">《Deep Learning》，I. Goodfellow, Y. Bengio and A. Courville</a>作为深度学习领域非常著名的一本书，虽然已经有些过时，但确实打基础的一本好书。笔者会从中挑选一些章节阅读并分享。</p><h1 id="深度前馈网络"><a href="#深度前馈网络" class="headerlink" title="深度前馈网络"></a>深度前馈网络</h1><p>绝大多数深度学习问题都可以总结为，给定一个输入 $x$，我们设计网络可以输出预期的 $y$。也就是说，我们期望可以学习到这个未知的映射 $f^*:x \rightarrow y&#x3D;f^*(x)$。深度前馈网络（deep feedforward network），也叫作前馈神经网络（feedforward neural network）或者多层感知机（multilayer perceptron, MLP），是典型的深度学习中近似映射的方法。前馈网络定义了一个映射 $y&#x3D;f(x,\theta)$，其中$\theta$为网络参数。</p><p>这里需要提一下，之所以称为前馈网络，是因为信息流从输入经过中间计算过程再到输出端，在模型的输出和模型本身之间没有反馈连接，当前馈神经网络被扩展成包含反馈连接时，它们被称为循环神经网络（recurrent neural network, RNN）。</p><p><img src="/2023/07/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/feedforward_NN.png" alt="前馈网络"></p><p>前馈网络的结果如上图所示。主要是由输出神经元，隐藏神经元，输出神经元。神经元之间的连线代表了信息传递的权重，每个隐藏神经元包含一个激活函数。显然神经网络隐藏层越多，深度越深，每个隐藏层的隐藏神经元越多，则宽度越大。</p><p>说完了基本的结构，其实对于深度前馈网络已经有个大概的了解了，但是对于深度学习这门课，动手实验要比看理论要更能学到东西，特别是调参的经验（也就是大家所调侃的炼丹经验）。</p><h1 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;随着alpha-go和Chatgpt的出现，人工智能（Artificial Intelligence，AI）展现出令人惊叹的发展前景，作为一</summary>
      
    
    
    
    <category term="深度学习" scheme="https://lovefamily-ren-wang.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="MLP" scheme="https://lovefamily-ren-wang.com/tags/MLP/"/>
    
  </entry>
  
</feed>
